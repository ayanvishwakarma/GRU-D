{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7439409,"sourceType":"datasetVersion","datasetId":4287453}],"dockerImageVersionId":30350,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Create New Conda Environment and Use Conda Channel \n!conda create -n newCondaEnvironment -c cctbx202208 -y &> /dev/null\n!source /opt/conda/bin/activate newCondaEnvironment && conda install -c cctbx202208 python=3.6 -y &> /dev/null\n!/opt/conda/envs/newCondaEnvironment/bin/python --version\n!sudo rm /opt/conda/bin/python\n!sudo ln -sf /opt/conda/envs/newCondaEnvironment/bin/python /opt/conda/bin/python\n!sudo rm /opt/conda/bin/python\n!sudo ln -sf /opt/conda/envs/newCondaEnvironment/bin/python /opt/conda/bin/python\n!sudo rm /opt/conda/bin/python\n!sudo ln -s /opt/conda/envs/newCondaEnvironment/bin/python /opt/conda/bin/python\n!python --version","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:20:37.608577Z","iopub.execute_input":"2024-02-06T21:20:37.609093Z","iopub.status.idle":"2024-02-06T21:20:49.223393Z","shell.execute_reply.started":"2024-02-06T21:20:37.609052Z","shell.execute_reply":"2024-02-06T21:20:49.221530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install jupyter &> /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:20:49.226353Z","iopub.execute_input":"2024-02-06T21:20:49.226835Z","iopub.status.idle":"2024-02-06T21:21:03.906933Z","shell.execute_reply.started":"2024-02-06T21:20:49.226789Z","shell.execute_reply":"2024-02-06T21:21:03.904828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import git\nimport os \nimport shutil\n\nos.chdir('/kaggle/working/')\n\nGITHUB_ACCESS_TOKEN = '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'\n\nif os.path.exists('GRU-D'):\n    shutil.rmtree('/kaggle/working/GRU-D')\n\ngit.Repo.clone_from(f'https://{GITHUB_ACCESS_TOKEN}@github.com/ayanvishwakarma/GRU-D.git', 'GRU-D')\n\nos.chdir('/kaggle/working/GRU-D')\nprint('Present working directory:',os.getcwd())\n\n!pip install -r requirements.txt --use-deprecated=legacy-resolver &> /dev/null\n\nif not os.path.exists('data/mimic3'):\n    os.mkdir('data/mimic3')\n    \nimport sys\nsys.path.append('/kaggle/working/GRU-D/Code')","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:21:03.909250Z","iopub.execute_input":"2024-02-06T21:21:03.909949Z","iopub.status.idle":"2024-02-06T21:21:18.372961Z","shell.execute_reply.started":"2024-02-06T21:21:03.909890Z","shell.execute_reply":"2024-02-06T21:21:18.371284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\nimport random\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_random_seed(seed)\n    np.random.seed(seed)\n    \ndef set_global_determinism(seed):\n    seed_everything(seed=seed)\n\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    \nset_global_determinism(2024)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:21:18.376845Z","iopub.execute_input":"2024-02-06T21:21:18.377314Z","iopub.status.idle":"2024-02-06T21:21:18.388935Z","shell.execute_reply.started":"2024-02-06T21:21:18.377271Z","shell.execute_reply":"2024-02-06T21:21:18.387407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nimport git\nimport os \nimport shutil\n\nos.chdir('/kaggle/working/')\n\nGITHUB_ACCESS_TOKEN = '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'\n\nif os.path.exists('SLAN'):\n    shutil.rmtree('/kaggle/working/SLAN')\n\ngit.Repo.clone_from(f'https://{GITHUB_ACCESS_TOKEN}@github.com/ayanvishwakarma/SLAN.git', 'SLAN')\n\nos.chdir('/kaggle/working/SLAN')\nprint('Present working directory:',os.getcwd())\n\n!pip install -r requirements.txt --use-deprecated=legacy-resolver &> /dev/null\n\nimport sys\nsys.path.append('/kaggle/working/SLAN/Code')\n\nfrom data import MIMICdata_Optimized\ntrain_dataset = MIMICdata_Optimized('Data/MIMIC/in-hospital-mortality/', 'Data/MIMIC/resources/', 'train', 'cpu', \n                 stat_file='Data/MIMIC/mimic_stat.json', sr_file='Data/MIMIC/sampling_rate_mimic.npy')\nval_dataset = MIMICdata_Optimized('Data/MIMIC/in-hospital-mortality/', 'Data/MIMIC/resources/', 'val', 'cpu', \n                 stat_file='Data/MIMIC/mimic_stat.json', sr_file='Data/MIMIC/sampling_rate_mimic.npy')\ntest_dataset = MIMICdata_Optimized('Data/MIMIC/in-hospital-mortality/', 'Data/MIMIC/resources/', 'test', 'cpu', \n                 stat_file='Data/MIMIC/mimic_stat.json', sr_file='Data/MIMIC/sampling_rate_mimic.npy')\n\ndata = {'timestamp': [((x['t'] - x['t'][0]) * 60 * 60).numpy() for dataset in [train_dataset, val_dataset, test_dataset] for x in dataset],\n    'masking': [np.logical_not(np.isnan(x['sensor_values'])).numpy() for dataset in [train_dataset, val_dataset, test_dataset] for x in dataset],\n    'input': [x['sensor_values'].numpy() for dataset in [train_dataset, val_dataset, test_dataset] for x in dataset],\n    'label_mortality': [x['y'].numpy() for dataset in [train_dataset, val_dataset, test_dataset] for x in dataset]}\n\nfold_info = {'fold_mortality': [[np.arange(len(train_dataset)), len(train_dataset) + np.arange(len(val_dataset)), \n                                len(train_dataset) + len(val_dataset) + np.arange(len(test_dataset))]], \n    'mean_mortality': [train_dataset.loaded_stat[x]['mean'] for x in train_dataset.discretizer_config['id_to_channel']],\n    'std_mortality': [train_dataset.loaded_stat[x]['std'] for x in train_dataset.discretizer_config['id_to_channel']]}\nfold_info['mean_mortality'] = [np.array([x if not x is None else np.nan for x in fold_info['mean_mortality']]), \n                              np.full(len(fold_info['mean_mortality']), fill_value=np.nan),\n                              np.full(len(fold_info['mean_mortality']), fill_value=np.nan)]\nfold_info['std_mortality'] = [np.array([x if not x is None else np.nan for x in fold_info['std_mortality']]),\n                             np.full(len(fold_info['std_mortality']), fill_value=np.nan),\n                             np.full(len(fold_info['std_mortality']), fill_value=np.nan)]\n\ndata = {key: np.array(value, dtype='O') for key, value in data.items()}\nfold_info = {key: np.array(value, dtype='O') for key, value in fold_info.items()}\n\nnp.savez_compressed(os.path.join('/kaggle/working/GRU-D/data/mimic3/', 'data.npz'), **data)\nnp.savez_compressed(os.path.join('/kaggle/working/GRU-D/data/mimic3/', 'fold.npz'), **fold_info)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:21:18.390679Z","iopub.execute_input":"2024-02-06T21:21:18.391354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nos.chdir('/kaggle/working/GRU-D')\nprint('Present working directory:',os.getcwd())\n\nimport argparse\nfrom datetime import datetime\nimport numpy as np\nimport os\n\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping, TensorBoard\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\nfrom data_handler import DataHandler\nfrom models import create_grud_model, load_grud_model\nfrom nn_utils.callbacks import ModelCheckpointwithBestWeights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set GPU usage for tensorflow backend\nif K.backend() == 'tensorflow':\n    import tensorflow as tf\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = .1\n    config.gpu_options.allow_growth = True\n    config.intra_op_parallelism_threads = 1\n    config.inter_op_parallelism_threads = 1\n    K.set_session(tf.Session(config=config))","metadata":{"execution":{"iopub.status.busy":"2024-02-06T20:09:31.578877Z","iopub.execute_input":"2024-02-06T20:09:31.579355Z","iopub.status.idle":"2024-02-06T20:09:31.594017Z","shell.execute_reply.started":"2024-02-06T20:09:31.579314Z","shell.execute_reply":"2024-02-06T20:09:31.592882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arg_parser = argparse.ArgumentParser()\narg_parser.add_argument('--working_path', default='.')\n\n## data\narg_parser.add_argument('dataset_name', default='mimic3',\n                        help='The data files should be saved in [working_path]/data/[dataset_name] directory.')\narg_parser.add_argument('label_name', default='mortality')\narg_parser.add_argument('--max_timesteps', type=int, default=200, \n                        help='Time series of at most # time steps are used. Default: 200.')\narg_parser.add_argument('--max_timestamp', type=int, default=48*60*60,\n                        help='Time series of at most # seconds are used. Default: 48 (hours).')\n\n## model\narg_parser.add_argument('--recurrent_dim', type=lambda x: x and [int(xx) for xx in x.split(',')] or [], default='64')\narg_parser.add_argument('--hidden_dim', type=lambda x: x and [int(xx) for xx in x.split(',')] or [], default='64')\narg_parser.add_argument('--model', default='GRUD', choices=['GRUD', 'GRUforward', 'GRU0', 'GRUsimple'])\narg_parser.add_argument('--use_bidirectional_rnn', default=False)\n                           \n## training\narg_parser.add_argument('--pretrained_model_file', default=None,\n                        help='If pre-trained model is provided, training will be skipped.') # e.g., [model_name]_[i_fold].h5\narg_parser.add_argument('--epochs', type=int, default=100)\narg_parser.add_argument('--early_stopping_patience', type=int, default=10)\narg_parser.add_argument('--batch_size', type=int, default=32)\n\n\n## set the actual arguments if running in notebook\nif not (__name__ == '__main__' and '__file__' in globals()):\n    ARGS = arg_parser.parse_args([\n        'mimic3',\n        'mortality',\n        '--model', 'GRUD',\n        '--hidden_dim', '',\n        '--epochs', '100'\n    ])\nelse:\n    ARGS = arg_parser.parse_args()\n\nprint('Arguments:', ARGS)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T20:45:01.377328Z","iopub.execute_input":"2024-02-06T20:45:01.377879Z","iopub.status.idle":"2024-02-06T20:45:01.398232Z","shell.execute_reply.started":"2024-02-06T20:45:01.377833Z","shell.execute_reply":"2024-02-06T20:45:01.397109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = DataHandler(\n    data_path=os.path.join('/kaggle/working/GRU-D', 'data', ARGS.dataset_name), \n    label_name=ARGS.label_name, \n    max_steps=ARGS.max_timesteps,\n    max_timestamp=ARGS.max_timestamp\n)\n\ndef weighted_bce(y_true, y_pred):\n    weights = y_true * 7.3975 + (1 - y_true) * 1.1563\n    bce = K.binary_crossentropy(y_true, y_pred)\n    weighted_bce = K.mean(bce * weights)\n    return weighted_bce\n\nloss_fn = weighted_bce","metadata":{"execution":{"iopub.status.busy":"2024-02-06T20:59:41.281486Z","iopub.execute_input":"2024-02-06T20:59:41.281971Z","iopub.status.idle":"2024-02-06T20:59:43.299035Z","shell.execute_reply.started":"2024-02-06T20:59:41.281922Z","shell.execute_reply":"2024-02-06T20:59:43.297637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# k-fold cross-validation\npred_y_list_all = []\nauc_score_list_all = []\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\nprint('Timestamp: {}'.format(timestamp))\n\nfor i_fold in range(dataset.folds):\n    print('{}-th fold...'.format(i_fold))\n\n    # Load or train the model.\n    if ARGS.pretrained_model_file is not None:\n        model = load_grud_model(os.path.join(ARGS.working_path, \n                                             ARGS.pretrained_model_file.format(i_fold=i_fold)))\n    else:\n        model = create_grud_model(input_dim=dataset.input_dim,\n                                  output_dim=dataset.output_dim,\n                                  output_activation=dataset.output_activation,\n                                  recurrent_dim=ARGS.recurrent_dim,\n                                  hidden_dim=ARGS.hidden_dim,\n                                  predefined_model=ARGS.model,\n                                  use_bidirectional_rnn=ARGS.use_bidirectional_rnn\n                                 )\n        if i_fold == 0:\n            model.summary()\n        model.compile(optimizer='adam', loss=loss_fn)#dataset.loss_function)\n        model.fit_generator(\n            generator=dataset.training_generator(i_fold, batch_size=ARGS.batch_size),\n            steps_per_epoch=dataset.training_steps(i_fold, batch_size=ARGS.batch_size),\n            epochs=ARGS.epochs,\n            verbose=1,\n            validation_data=dataset.validation_generator(i_fold, batch_size=ARGS.batch_size),\n            validation_steps=dataset.validation_steps(i_fold, batch_size=ARGS.batch_size),\n            callbacks=[\n                EarlyStopping(patience=ARGS.early_stopping_patience),\n                ModelCheckpointwithBestWeights(\n                    file_dir=os.path.join(ARGS.working_path, 'model', timestamp + '_' + str(i_fold))\n                ),\n                TensorBoard(\n                    log_dir=os.path.join(ARGS.working_path, 'tb_logs', timestamp + '_' + str(i_fold))\n                )\n            ]\n            )\n        model.save(os.path.join(ARGS.working_path, 'model', \n                                timestamp + '_' + str(i_fold), 'model.h5'))\n\n    # Evaluate the model\n    true_y_list = [\n        dataset.training_y(i_fold), dataset.validation_y(i_fold), dataset.testing_y(i_fold)\n    ]\n    pred_y_list = [\n        model.predict_generator(dataset.training_generator_x(i_fold, batch_size=ARGS.batch_size),\n                                steps=dataset.training_steps(i_fold, batch_size=ARGS.batch_size)),\n        model.predict_generator(dataset.validation_generator_x(i_fold, batch_size=ARGS.batch_size),\n                                steps=dataset.validation_steps(i_fold, batch_size=ARGS.batch_size)),\n        model.predict_generator(dataset.testing_generator_x(i_fold, batch_size=ARGS.batch_size),\n                                steps=dataset.testing_steps(i_fold, batch_size=ARGS.batch_size)),\n    ]\n    auc_score_list = [roc_auc_score(np.stack(ty), py, average='macro') for ty, py in zip(true_y_list, pred_y_list)] # [3, n_task]\n    aurpc_score_list= [average_precision_score(np.stack(ty), py) for ty, py in zip(true_y_list, pred_y_list)]\n    print('AUC score of this fold: {}'.format(auc_score_list))\n    print('AUPRC score of this fold: {}'.format(aurpc_score_list))\n    pred_y_list_all.append(pred_y_list)\n    auc_score_list_all.append(auc_score_list)\n\nprint('Finished!', '='*20)\nauc_score_list_all = np.stack(auc_score_list_all, axis=0)\nprint('Mean AUC score: {}; Std AUC score: {}'.format(\n    np.mean(auc_score_list_all, axis=0),\n    np.std(auc_score_list_all, axis=0)))\n\nresult_path = os.path.join(ARGS.working_path, 'results', timestamp)\nif not os.path.exists(result_path):\n    os.makedirs(result_path)\nnp.savez_compressed(os.path.join(result_path, 'predictions.npz'),\n                    pred_y_list_all=pred_y_list_all)\nnp.savez_compressed(os.path.join(result_path, 'auroc_score.npz'),\n                    auc_score_list_all=auc_score_list_all)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T20:59:43.302795Z","iopub.execute_input":"2024-02-06T20:59:43.303428Z","iopub.status.idle":"2024-02-06T21:11:42.600494Z","shell.execute_reply.started":"2024-02-06T20:59:43.303388Z","shell.execute_reply":"2024-02-06T21:11:42.599360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}